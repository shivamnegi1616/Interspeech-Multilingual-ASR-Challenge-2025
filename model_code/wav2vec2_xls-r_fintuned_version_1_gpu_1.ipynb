{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869e76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jiwer import wer\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import Audio\n",
    "# AdamW is best optimizer\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee618a0",
   "metadata": {},
   "source": [
    "### Data Making (Do Not Run this Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/data/set1_set2_set3_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b290d3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1398574, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169dab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class column name\n",
    "class_column = 'class'  # Replace with actual class column name\n",
    "\n",
    "# Shuffle each class group independently\n",
    "class_groups = {cls: df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "                for cls, df in full_data.groupby(class_column)}\n",
    "\n",
    "# Determine how many total rows and chunks we want\n",
    "total_rows = len(full_data)\n",
    "chunk_size = 100_000\n",
    "n_chunks = (total_rows + chunk_size - 1) // chunk_size  # round up\n",
    "\n",
    "# Prepare empty list to collect rows\n",
    "balanced_data = []\n",
    "\n",
    "# Interleave rows in balanced way\n",
    "for chunk_idx in range(n_chunks):\n",
    "    chunk = []\n",
    "    chunk_start = chunk_idx * chunk_size\n",
    "    chunk_end = min((chunk_idx + 1) * chunk_size, total_rows)\n",
    "\n",
    "    rows_needed = chunk_end - chunk_start\n",
    "    per_class_rows = rows_needed // len(class_groups)\n",
    "\n",
    "    for cls, df in class_groups.items():\n",
    "        take = min(per_class_rows, len(df))\n",
    "        chunk.append(df.iloc[:take])\n",
    "        class_groups[cls] = df.iloc[take:].reset_index(drop=True)\n",
    "\n",
    "    # Handle remainder rows (to reach 100k)\n",
    "    remainder = rows_needed - sum(len(c) for c in chunk)\n",
    "    if remainder > 0:\n",
    "        # Grab remainder rows randomly from what's left\n",
    "        leftovers = pd.concat([df for df in class_groups.values() if not df.empty])\n",
    "        extra = leftovers.sample(n=remainder, random_state=chunk_idx)\n",
    "        chunk.append(extra)\n",
    "\n",
    "    balanced_data.append(pd.concat(chunk).sample(frac=1, random_state=chunk_idx).reset_index(drop=True))\n",
    "\n",
    "# Final combined dataframe\n",
    "final_data = pd.concat(balanced_data).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35a84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "english    98574\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class distribution in the first 100,000 rows\n",
    "final_data.loc[1300000:, 'class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14973feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "english       9092\n",
       "korean        9092\n",
       "thai          9092\n",
       "spanish       9092\n",
       "russian       9092\n",
       "japanese      9091\n",
       "portuguese    9090\n",
       "french        9090\n",
       "german        9090\n",
       "italian       9090\n",
       "vietnamese    9090\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.loc[:100000, 'class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c5ed2d",
   "metadata": {},
   "source": [
    "Now we have equal class distribution on each subsequent 100000 lakh samples of the now we will use these chunks to train the model, last 1 lakh chunk only have english smaples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6888a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "italian       9092\n",
       "english       9092\n",
       "vietnamese    9092\n",
       "russian       9092\n",
       "spanish       9092\n",
       "french        9091\n",
       "japanese      9090\n",
       "thai          9090\n",
       "portuguese    9090\n",
       "german        9090\n",
       "korean        9090\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.loc[100000:200000, 'class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d7bf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/data/balanced_shuffled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc572bf6",
   "metadata": {},
   "source": [
    "### Config (Only Modify this then run all below this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac81b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"BASE_MODEL_ID\" : \"facebook/wav2vec2-xls-r-2b\",\n",
    "            \n",
    "            \"start_data\" : 200000,\n",
    "            \"end_data\" : 300000,\n",
    "            'device' : 1,\n",
    "            'BATCH_SIZE' : 16,\n",
    "            'EPOCHS' : 30,\n",
    "            'LR' : 1e-5,\n",
    "            \n",
    "            'Number_of_first_layers_freeze_transofrmer' : 48,\n",
    "            \n",
    "            'vocab_json_path' : \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_files/multilingual_vocab.json\",\n",
    "            \n",
    "            'num_warmup_steps' : 1000,\n",
    "            \n",
    "            'loss_csv_saving_path'  : \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_loss_files/loss_2_3_lakh\",\n",
    "            \n",
    "            'loss_csv_name' : \"loss_2_3_lakh.csv\",\n",
    "             \n",
    "            'prev_checkpoint_dir' : \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_weights/weights_0_2_lakh/multilingual_asr_model_0_2_lakh.pt\",\n",
    "            \n",
    "            'new_checkpoint_dir' : \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_weights/weights_2_3_lakh\",\n",
    "            \n",
    "            'model_name' : \"multilingual_asr_model_2_3_lakh\",\n",
    "            \n",
    "            'load_from_prev' : True,\n",
    "            'resume_training' : False\n",
    "          \n",
    "          \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2225a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{config['device']}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613d8020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1634d",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Do Run From this Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f416590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52fb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv('/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/data/scratch_balanced_shuffled_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1f73e",
   "metadata": {},
   "source": [
    "Now we will take 2 lakh  to 4 lakh sample for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9756676",
   "metadata": {},
   "source": [
    "lower case only the english text along the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb221b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.loc[config['start_data'] : config['end_data'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc64cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_english_letters(text):\n",
    "    return ''.join([c.lower() if c.isascii() and c.isalpha() else c for c in text])\n",
    "\n",
    "final_data['Text'] = final_data['Text'].apply(lowercase_english_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e58b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    characters_to_remove = r'[,\\?\\.\\!\\-\\;\\:\\%\\'\\`\\{}()@#$%^&*\\+\\[\\]\\_ï½¡]+'\n",
    "    cleaned_text = re.sub(characters_to_remove, '', text)\n",
    "    # cleaned_text = ''.join([c.lower() if c.isascii() else c for c in cleaned_text]) + ' '\n",
    "    return cleaned_text\n",
    "\n",
    "final_data['Text'] = final_data['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99676b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "english       9094\n",
       "korean        9091\n",
       "vietnamese    9091\n",
       "thai          9091\n",
       "spanish       9091\n",
       "russian       9091\n",
       "french        9091\n",
       "japanese      9091\n",
       "german        9090\n",
       "italian       9090\n",
       "portuguese    9090\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a138a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f19933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b496585",
   "metadata": {},
   "source": [
    "### Building Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56e8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(r\"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_files/multilingual_vocab.json\", bos_token = \"<s>\",\n",
    "                                 eos_token = \"</s>\",\n",
    "                                 unk_token = \"<unk>\", \n",
    "                                 pad_token = \"<pad>\", \n",
    "                                 word_delimiter_token = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392ae07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size = 1, \n",
    "                                             sampling_rate = 16000, \n",
    "                                             padding_value = 0.0, \n",
    "                                             do_normalize = True, \n",
    "                                             return_attention_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9bfb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor = feature_extractor, \n",
    "                              tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "569ba5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech = speech.squeeze()\n",
    "\n",
    "# input_values = processor.feature_extractor(speech, sampling_rate = 16000, return_tensors=\"pt\")\n",
    "\n",
    "# labels = processor.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a94dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)  # Convert to MB\n",
    "# print(f\"Model size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748eb9a5",
   "metadata": {},
   "source": [
    "### Data Class \\& Data Loader Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb277818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assume df is your DataFrame and 'class' is the column with class labels\n",
    "# train_df, test_df = train_test_split(\n",
    "#                                         full_data,\n",
    "#                                         test_size = 0.5,\n",
    "#                                         stratify = full_data['class'],      # Ensures equal class distribution\n",
    "#                                         random_state = 42            # For reproducibility\n",
    "#                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1245d96",
   "metadata": {},
   "source": [
    "Currently i am using only 50 percent of data for training to see wether the wer decresing or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec32c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.reset_index(drop = True)\n",
    "# test_df = test_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b2020d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5af32bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['class'].value_counts(), test_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa165c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if index is not in sequential order\n",
    "# mismatched_indices = train_df.index != range(len(train_df))\n",
    "\n",
    "# # Show the rows where index is not in order\n",
    "# train_df[mismatched_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58b21be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, df, processor, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)  # ensure index is clean\n",
    "        self.processor = processor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        waveform, sample_rate = torchaudio.load(row['Path'])\n",
    "        text = row['Text']\n",
    "\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        waveform = waveform.squeeze()\n",
    "\n",
    "        # Optional audio transforms\n",
    "        if self.transforms:\n",
    "            for transform in self.transforms:\n",
    "                waveform = transform(waveform)\n",
    "\n",
    "        # Process audio\n",
    "        input_values = self.processor.feature_extractor(\n",
    "                                                        waveform, sampling_rate=16000, return_tensors=\"pt\"\n",
    "                                                    )[\"input_values\"].squeeze(0)\n",
    "\n",
    "        # Process labels (text)\n",
    "        labels = self.processor.tokenizer(\n",
    "                                            text, return_tensors=\"pt\", truncation=True\n",
    "                                        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f7f7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_dataset = SpeechDataset(final_data, processor, transforms = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2590f2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speech_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82e7cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.loc[957005, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3cef067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = speech_dataset[45]\n",
    "# audio = sample[\"input_values\"]\n",
    "# transcription = sample[\"labels\"]\n",
    "\n",
    "# print(audio)\n",
    "# print(audio.shape)\n",
    "# print(transcription)\n",
    "# print(transcription.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b60697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch, processor, \n",
    "                     padding = True, \n",
    "                     max_length = None, \n",
    "                     max_length_labels = None, \n",
    "                     pad_to_multiple_of = None, \n",
    "                     pad_to_multiple_of_labels = None):\n",
    "  \n",
    "    # Extract input values and labels from each sample in the batch\n",
    "    b_X = [{\"input_values\": sample[\"input_values\"]} for sample in batch]\n",
    "    b_Y = [{\"input_ids\": sample[\"labels\"]} for sample in batch]\n",
    "\n",
    "    # Pad the audio inputs the same length\n",
    "    features = processor.feature_extractor.pad(\n",
    "                                                b_X,\n",
    "                                                padding = padding,\n",
    "                                                max_length = max_length,\n",
    "                                                pad_to_multiple_of = pad_to_multiple_of,\n",
    "                                                return_tensors = \"pt\"\n",
    "                                              )\n",
    "\n",
    "    # Pad the labels\n",
    "    batchY = processor.tokenizer.pad(\n",
    "                            b_Y,\n",
    "                            padding = padding,\n",
    "                            max_length = max_length_labels,\n",
    "                            pad_to_multiple_of = pad_to_multiple_of_labels,\n",
    "                            return_tensors = \"pt\"\n",
    "                          )\n",
    "\n",
    "    # Replace padding tokens in labels with -100, so they are ignored during loss calculation\n",
    "    labels = batchY[\"input_ids\"].masked_fill(batchY.attention_mask.ne(1), -100)\n",
    "\n",
    "    # Add the padded labels back into the features dictionary\n",
    "    features[\"labels\"] = labels\n",
    "\n",
    "    # Return the features, which now include both input values and labels\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51545589",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = partial(collate_function, \n",
    "                     processor = processor, \n",
    "                     padding = True, \n",
    "                     max_length = None, \n",
    "                     max_length_labels = None, \n",
    "                     pad_to_multiple_of = None, \n",
    "                     pad_to_multiple_of_labels = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98026d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['BATCH_SIZE']\n",
    "epochs = config['EPOCHS']\n",
    "lr = config['LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eaa6019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 30, 1e-05)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, epochs, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "273d1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(speech_dataset, \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle = True, \n",
    "                              collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7161230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_speech_data = SpeechDataset(test_df, processor, transforms = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "201c7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = DataLoader(test_speech_data, \n",
    "#                               batch_size = batch_size, \n",
    "#                               shuffle = True, \n",
    "#                               collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa3e0cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128000])\n",
      "torch.Size([16, 128000])\n",
      "torch.Size([16, 131])\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i['input_values'].shape)\n",
    "    print(i['attention_mask'].shape)\n",
    "    print(i['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6303e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test_dataloader:\n",
    "#     print(i['input_values'].shape)\n",
    "#     print(i['attention_mask'].shape)\n",
    "#     print(i['labels'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807ded8",
   "metadata": {},
   "source": [
    "### Loading the Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45b2caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2Model.from_pretrained(config['BASE_MODEL_ID'])\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec5c3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods = [method for method in dir(model) if callable(getattr(model, method))]\n",
    "# print(methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a99eb707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "    (attention): Wav2Vec2SdpaAttention(\n",
       "      (k_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "      (v_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "      (q_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "      (out_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "    (feed_forward): Wav2Vec2FeedForward(\n",
       "      (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (intermediate_dense): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (output_dense): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a2aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 48 transformer layers in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267cb74",
   "metadata": {},
   "source": [
    "### Currently i am training only My custom CTC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57da4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze feature extractor (same as model.freeze_feature_encoder())\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze feature projection (optional)\n",
    "for param in model.feature_projection.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze all transformer encoder layers\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "    \n",
    "# Freeze first 30 transformer layers (out of 48 for base model)\n",
    "# for i in range(config['Number_of_first_layers_freeze_transofrmer']):\n",
    "#     for param in model.encoder.layers[i].parameters():\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "        \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Optional: Print which layers are frozen\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb398821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 0 / Total: 2159259648\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable_params} / Total: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f870766",
   "metadata": {},
   "source": [
    "### Building the custom CTC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68121554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, model, projection_dim = 5000):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = model\n",
    "        self.projection = nn.Linear(1920, projection_dim)\n",
    "\n",
    "    def forward(self, input_values, attention_mask = None):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask = attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [batch, time, hidden]\n",
    "        projected = self.projection(hidden_states)  # [batch, time, 5000]\n",
    "        return projected\n",
    "\n",
    "\n",
    "# Custom CTC model\n",
    "class CustomWav2Vec2CTC(nn.Module):\n",
    "    def __init__(self, model, vocab_size, projection_dim = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.projector = Projector(model, projection_dim = projection_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(projection_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_values, attention_mask = None):\n",
    "        hidden_states = self.projector(input_values, attention_mask)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d17b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afd4bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3147"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = config['vocab_json_path']\n",
    "\n",
    "# Open and load JSON data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ce38133",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3147  # your vocab size here\n",
    "\n",
    "# This is complete model\n",
    "\n",
    "multilingual_asr_model = CustomWav2Vec2CTC(model, vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a928fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 25343147 / Total: 2184602795\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in multilingual_asr_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in multilingual_asr_model.parameters())\n",
    "print(f\"Trainable: {trainable_params} / Total: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce1869",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e57ce191",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(multilingual_asr_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c14f9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of training steps the model will take during the entire training process.\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_scheduler\n",
    "# SchedulerType, please select one of ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau', 'cosine_with_min_lr', 'warmup_stable_decay']\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "                            \"linear\",\n",
    "                            optimizer = optimizer,\n",
    "                            num_warmup_steps = config['num_warmup_steps'],\n",
    "                            num_training_steps = num_training_steps\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84127811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187530"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mixed training of torch.float32, torch.float64\n",
    "# https://pytorch.org/docs/stable/amp.html\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "multilingual_asr_model = multilingual_asr_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40c5d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "ctc_loss_fn = nn.CTCLoss(blank = processor.tokenizer.pad_token_id, zero_infinity = True, reduction = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555d911",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cb8a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels, preds):\n",
    "    \n",
    "    # preds = torch.argmax(preds, axis=-1)\n",
    "\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # print('The shape of the preds is', preds)\n",
    "    # print('The shape of the label is', labels)\n",
    "    pred_str = processor.batch_decode(preds)\n",
    "    \n",
    "    # print('The pred str is', pred_str)\n",
    "    label_str = processor.batch_decode(labels, group_tokens = False)\n",
    "    # print('The label str is', label_str)\n",
    "    return wer(label_str, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01f4a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define CSV directory and file\n",
    "csv_dir = config['loss_csv_saving_path']\n",
    "csv_file_name = config['loss_csv_name']\n",
    "csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# If the CSV file doesn't exist, create it with headers\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"step\", \"epoch\", \"loss\", \"wer\"])\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08a408",
   "metadata": {},
   "source": [
    "### Checkpoint saving variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f93d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded form the previous model\n",
      "Resumed training from epoch 10 with best loss 1000000000.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "new_checkpoint_dir = config['new_checkpoint_dir']\n",
    "\n",
    "os.makedirs(new_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "start_epoch = 0\n",
    "curr_best_loss = 1e9\n",
    "\n",
    "resume_training = config['resume_training']  # set this to False to start fresh\n",
    "\n",
    "\n",
    "model_name = config['model_name']\n",
    "\n",
    "new_checkpoint_path = os.path.join(new_checkpoint_dir, model_name + \".pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if config['load_from_prev']:\n",
    "    \n",
    "    checkpoint_path = config['prev_checkpoint_dir']\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location = 'cpu', weights_only = False)\n",
    "    \n",
    "    multilingual_asr_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    curr_best_loss = checkpoint.get('best_loss', curr_best_loss)\n",
    "    \n",
    "    print('Loaded form the previous model')\n",
    "    print(f\"Resumed training from epoch {start_epoch} with best loss {curr_best_loss:.4f}\")\n",
    "    \n",
    "\n",
    "# Resume logic and save in new check dir since it is new chunk of the data\n",
    "if resume_training and os.path.exists(new_checkpoint_path):\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    multilingual_asr_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    curr_best_loss = checkpoint.get('best_loss', curr_best_loss)\n",
    "    \n",
    "    print(f\"Resumed training from epoch {start_epoch} with best loss {curr_best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c55b0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78489ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_asr_model = multilingual_asr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8a512f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move optimizer state to the correct device\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f677124642c44459beca874aa7ee6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a9b53025ee46e3b86b771af6704859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  11\n",
      "{'loss': np.float64(3.815136089990128), 'wer': np.float64(1.0008730460704365)}\n",
      "====================================================================================================\n",
      "New best model found at epoch 11 with loss 3.8151, saving...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:626] . unexpected pos 6878457472 vs 6878457368",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/shivam_env/lib/python3.13/site-packages/torch/serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/shivam_env/lib/python3.13/site-packages/torch/serialization.py:1216\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1215\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/680: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# Save latest checkpoint\u001b[39;00m\n\u001b[32m     86\u001b[39m     checkpoint = {\n\u001b[32m     87\u001b[39m                         \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m: n + \u001b[32m1\u001b[39m,\n\u001b[32m     88\u001b[39m                         \u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m: multilingual_asr_model.state_dict(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m                         \u001b[33m'\u001b[39m\u001b[33mbest_loss\u001b[39m\u001b[33m'\u001b[39m: curr_best_loss\n\u001b[32m     93\u001b[39m                     }\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_checkpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining completed.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/shivam_env/lib/python3.13/site-packages/torch/serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/shivam_env/lib/python3.13/site-packages/torch/serialization.py:784\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:626] . unexpected pos 6878457472 vs 6878457368"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "curr_best_loss = 1e9\n",
    "\n",
    "for n in tqdm(range(start_epoch, epochs)):\n",
    "    \n",
    "    losses = []\n",
    "    wers = []\n",
    "\n",
    "    total_number_batch = len(train_dataloader)\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            logits = multilingual_asr_model(batch[\"input_values\"], attention_mask = batch.get(\"attention_mask\"))\n",
    "            \n",
    "        log_probs = F.log_softmax(logits, dim = -1).transpose(0, 1)\n",
    "\n",
    "        input_lengths = torch.full(\n",
    "                                    size = (log_probs.size(1),),  # batch size\n",
    "                                    fill_value = log_probs.size(0),  # time dimension\n",
    "                                    dtype = torch.long).to(device)\n",
    "\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        target_lengths = (labels != -100).sum(dim = 1)\n",
    "\n",
    "        flattened_targets = labels[labels != -100]\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled = False):\n",
    "            loss = ctc_loss_fn(\n",
    "                                log_probs.float(),         # ensure float32\n",
    "                                flattened_targets,\n",
    "                                input_lengths,\n",
    "                                target_lengths\n",
    "                                )\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        lr_scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "        # WER computation\n",
    "        preds = torch.argmax(log_probs, dim = -1).transpose(0, 1)  # back to (batch, time)\n",
    "        \n",
    "        labels_for_metrics = labels.clone()\n",
    "        \n",
    "        metrics = compute_metrics(labels_for_metrics, preds)\n",
    "        wers.append(metrics)\n",
    "        \n",
    "        \n",
    "        with open(csv_file_path, mode = 'a', newline = '') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"step\", \"epoch\", \"loss\", \"wer\"])\n",
    "            \n",
    "            writer.writerow({\n",
    "                                \"step\": step,\n",
    "                                \"epoch\": n + 1,\n",
    "                                \"loss\": loss.item(),\n",
    "                                \"wer\": metrics\n",
    "                            })\n",
    "\n",
    "\n",
    "    result = {\"loss\": np.mean(losses), \"wer\": np.mean(wers)}    \n",
    "    \n",
    "    print(\"EPOCH: \", n + 1)\n",
    "    print(result)\n",
    "    print('=' * 100)\n",
    "    \n",
    "        \n",
    "    if result[\"loss\"] < curr_best_loss:\n",
    "        \n",
    "        print(f\"New best model found at epoch {n + 1} with loss {result['loss']:.4f}, saving...\")\n",
    "        \n",
    "        curr_best_loss = result[\"loss\"]\n",
    "    \n",
    "    \n",
    "    # Save latest checkpoint\n",
    "    checkpoint = {\n",
    "                        'epoch': n + 1,\n",
    "                        'model_state_dict': multilingual_asr_model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                        'loss': result['loss'],\n",
    "                        'best_loss': curr_best_loss\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    torch.save(checkpoint, new_checkpoint_path)\n",
    "\n",
    "print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b9c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shivam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

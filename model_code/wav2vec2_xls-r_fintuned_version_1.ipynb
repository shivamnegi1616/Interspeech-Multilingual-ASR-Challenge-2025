{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869e76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jiwer import wer\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import Audio\n",
    "# AdamW is best optimizer\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7fbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"facebook/wav2vec2-xls-r-2b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee618a0",
   "metadata": {},
   "source": [
    "### Data Making (Do Not Run From this Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/data/set1_set2_set3_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b290d3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1398574, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169dab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class column name\n",
    "class_column = 'class'  # Replace with actual class column name\n",
    "\n",
    "# Shuffle each class group independently\n",
    "class_groups = {cls: df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "                for cls, df in full_data.groupby(class_column)}\n",
    "\n",
    "# Determine how many total rows and chunks we want\n",
    "total_rows = len(full_data)\n",
    "chunk_size = 100_000\n",
    "n_chunks = (total_rows + chunk_size - 1) // chunk_size  # round up\n",
    "\n",
    "# Prepare empty list to collect rows\n",
    "balanced_data = []\n",
    "\n",
    "# Interleave rows in balanced way\n",
    "for chunk_idx in range(n_chunks):\n",
    "    chunk = []\n",
    "    chunk_start = chunk_idx * chunk_size\n",
    "    chunk_end = min((chunk_idx + 1) * chunk_size, total_rows)\n",
    "\n",
    "    rows_needed = chunk_end - chunk_start\n",
    "    per_class_rows = rows_needed // len(class_groups)\n",
    "\n",
    "    for cls, df in class_groups.items():\n",
    "        take = min(per_class_rows, len(df))\n",
    "        chunk.append(df.iloc[:take])\n",
    "        class_groups[cls] = df.iloc[take:].reset_index(drop=True)\n",
    "\n",
    "    # Handle remainder rows (to reach 100k)\n",
    "    remainder = rows_needed - sum(len(c) for c in chunk)\n",
    "    if remainder > 0:\n",
    "        # Grab remainder rows randomly from what's left\n",
    "        leftovers = pd.concat([df for df in class_groups.values() if not df.empty])\n",
    "        extra = leftovers.sample(n=remainder, random_state=chunk_idx)\n",
    "        chunk.append(extra)\n",
    "\n",
    "    balanced_data.append(pd.concat(chunk).sample(frac=1, random_state=chunk_idx).reset_index(drop=True))\n",
    "\n",
    "# Final combined dataframe\n",
    "final_data = pd.concat(balanced_data).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35a84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "english    98574\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class distribution in the first 100,000 rows\n",
    "final_data.loc[1300000:, 'class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14973feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "english       9092\n",
       "korean        9092\n",
       "thai          9092\n",
       "spanish       9092\n",
       "russian       9092\n",
       "japanese      9091\n",
       "portuguese    9090\n",
       "french        9090\n",
       "german        9090\n",
       "italian       9090\n",
       "vietnamese    9090\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.loc[:100000, 'class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c5ed2d",
   "metadata": {},
   "source": [
    "Now we have equal class distribution on each subsequent 100000 lakh samples of the now we will use these chunks to train the model, last 1 lakh chunk only have english smaples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6888a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "italian       9092\n",
       "english       9092\n",
       "vietnamese    9092\n",
       "russian       9092\n",
       "spanish       9092\n",
       "french        9091\n",
       "japanese      9090\n",
       "thai          9090\n",
       "portuguese    9090\n",
       "german        9090\n",
       "korean        9090\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.loc[100000:200000, 'class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d7bf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/data/balanced_shuffled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1634d",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Do Run From this Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b52fb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv('/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/data/balanced_shuffled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5257604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1398574, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9756676",
   "metadata": {},
   "source": [
    "lower case only the english text along the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc64cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_english_letters(text):\n",
    "    return ''.join([c.lower() if c.isascii() and c.isalpha() else c for c in text])\n",
    "\n",
    "final_data['Text'] = final_data['Text'].apply(lowercase_english_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e58b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    characters_to_remove = r'[,\\?\\.\\!\\-\\;\\:\\%\\'\\`\\{}()@#$%^&*\\+\\[\\]\\_｡]+'\n",
    "    cleaned_text = re.sub(characters_to_remove, '', text)\n",
    "    # cleaned_text = ''.join([c.lower() if c.isascii() else c for c in cleaned_text]) + ' '\n",
    "    return cleaned_text\n",
    "\n",
    "final_data['Text'] = final_data['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99676b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "portuguese    2368\n",
      "russian       2298\n",
      "korean        2289\n",
      "french        2288\n",
      "thai          2287\n",
      "vietnamese    2274\n",
      "italian       2262\n",
      "german        2258\n",
      "english       2251\n",
      "japanese      2213\n",
      "spanish       2213\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_data.loc[:25000, 'class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc76bd",
   "metadata": {},
   "source": [
    "### Now we will take first 2 lakh sample for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee27086",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2lakh = final_data.loc[:200000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dca44678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200001, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2lakh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61f0eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "english       18184\n",
       "russian       18184\n",
       "spanish       18184\n",
       "vietnamese    18182\n",
       "korean        18182\n",
       "thai          18182\n",
       "italian       18182\n",
       "french        18181\n",
       "portuguese    18180\n",
       "japanese      18180\n",
       "german        18180\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2lakh['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4c4df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_2lakh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = train_data_2lakh.loc[199, 'Path']\n",
    "text = train_data_2lakh.loc[199, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/data/audio_set_2_chunks/japanese/chunk_2/0109-0109_005_phone-O1-084334-084589.wav',\n",
       " '年配の人でもやってるの？たまに見ますよね。')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b496585",
   "metadata": {},
   "source": [
    "### Building Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56e8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(r\"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_files/multilingual_vocab.json\", bos_token = \"<s>\",\n",
    "                                 eos_token = \"</s>\",\n",
    "                                 unk_token = \"<unk>\", \n",
    "                                 pad_token = \"<pad>\", \n",
    "                                 word_delimiter_token = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cb30d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ad1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech, sr = torchaudio.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39217cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech.shape, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f343748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['hello', 'worlss', 'i love you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc3d7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_values = tokenizer(text,return_tensors=\"pt\", padding = True, truncation = True, max_length=512)\n",
    "# input_values\n",
    "# tokenizer.decode(input_values['input_ids'][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392ae07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size = 1, \n",
    "                                             sampling_rate = 16000, \n",
    "                                             padding_value = 0.0, \n",
    "                                             do_normalize = True, \n",
    "                                             return_attention_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9bfb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor = feature_extractor, \n",
    "                              tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "569ba5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech = speech.squeeze()\n",
    "\n",
    "# input_values = processor.feature_extractor(speech, sampling_rate = 16000, return_tensors=\"pt\")\n",
    "\n",
    "# labels = processor.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a94dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)  # Convert to MB\n",
    "# print(f\"Model size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748eb9a5",
   "metadata": {},
   "source": [
    "### Data Class \\& Data Loader Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb277818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assume df is your DataFrame and 'class' is the column with class labels\n",
    "# train_df, test_df = train_test_split(\n",
    "#                                         full_data,\n",
    "#                                         test_size = 0.5,\n",
    "#                                         stratify = full_data['class'],      # Ensures equal class distribution\n",
    "#                                         random_state = 42            # For reproducibility\n",
    "#                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1245d96",
   "metadata": {},
   "source": [
    "Currently i am using only 50 percent of data for training to see wether the wer decresing or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec32c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.reset_index(drop = True)\n",
    "# test_df = test_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b2020d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5af32bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['class'].value_counts(), test_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa165c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if index is not in sequential order\n",
    "# mismatched_indices = train_df.index != range(len(train_df))\n",
    "\n",
    "# # Show the rows where index is not in order\n",
    "# train_df[mismatched_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58b21be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, df, processor, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)  # ensure index is clean\n",
    "        self.processor = processor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        waveform, sample_rate = torchaudio.load(row['Path'])\n",
    "        text = row['Text']\n",
    "\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        waveform = waveform.squeeze()\n",
    "\n",
    "        # Optional audio transforms\n",
    "        if self.transforms:\n",
    "            for transform in self.transforms:\n",
    "                waveform = transform(waveform)\n",
    "\n",
    "        # Process audio\n",
    "        input_values = self.processor.feature_extractor(\n",
    "                                                        waveform, sampling_rate=16000, return_tensors=\"pt\"\n",
    "                                                    )[\"input_values\"].squeeze(0)\n",
    "\n",
    "        # Process labels (text)\n",
    "        labels = self.processor.tokenizer(\n",
    "                                            text, return_tensors=\"pt\", truncation=True\n",
    "                                        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f7f7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_dataset = SpeechDataset(train_data_2lakh, processor, transforms = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2590f2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speech_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82e7cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.loc[957005, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3cef067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = speech_dataset[45]\n",
    "# audio = sample[\"input_values\"]\n",
    "# transcription = sample[\"labels\"]\n",
    "\n",
    "# print(audio)\n",
    "# print(audio.shape)\n",
    "# print(transcription)\n",
    "# print(transcription.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b60697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch, processor, \n",
    "                     padding = True, \n",
    "                     max_length = None, \n",
    "                     max_length_labels = None, \n",
    "                     pad_to_multiple_of = None, \n",
    "                     pad_to_multiple_of_labels = None):\n",
    "  \n",
    "    # Extract input values and labels from each sample in the batch\n",
    "    b_X = [{\"input_values\": sample[\"input_values\"]} for sample in batch]\n",
    "    b_Y = [{\"input_ids\": sample[\"labels\"]} for sample in batch]\n",
    "\n",
    "    # Pad the audio inputs the same length\n",
    "    features = processor.feature_extractor.pad(\n",
    "                                                b_X,\n",
    "                                                padding = padding,\n",
    "                                                max_length = max_length,\n",
    "                                                pad_to_multiple_of = pad_to_multiple_of,\n",
    "                                                return_tensors = \"pt\"\n",
    "                                              )\n",
    "\n",
    "    # Pad the labels\n",
    "    batchY = processor.tokenizer.pad(\n",
    "                            b_Y,\n",
    "                            padding = padding,\n",
    "                            max_length = max_length_labels,\n",
    "                            pad_to_multiple_of = pad_to_multiple_of_labels,\n",
    "                            return_tensors = \"pt\"\n",
    "                          )\n",
    "\n",
    "    # Replace padding tokens in labels with -100, so they are ignored during loss calculation\n",
    "    labels = batchY[\"input_ids\"].masked_fill(batchY.attention_mask.ne(1), -100)\n",
    "\n",
    "    # Add the padded labels back into the features dictionary\n",
    "    features[\"labels\"] = labels\n",
    "\n",
    "    # Return the features, which now include both input values and labels\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51545589",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = partial(collate_function, \n",
    "                     processor = processor, \n",
    "                     padding = True, \n",
    "                     max_length = None, \n",
    "                     max_length_labels = None, \n",
    "                     pad_to_multiple_of = None, \n",
    "                     pad_to_multiple_of_labels = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98026d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 30\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "273d1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(speech_dataset, \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle = True, \n",
    "                              collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7161230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_speech_data = SpeechDataset(test_df, processor, transforms = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "201c7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = DataLoader(test_speech_data, \n",
    "#                               batch_size = batch_size, \n",
    "#                               shuffle = True, \n",
    "#                               collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa3e0cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 125440])\n",
      "torch.Size([16, 125440])\n",
      "torch.Size([16, 92])\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i['input_values'].shape)\n",
    "    print(i['attention_mask'].shape)\n",
    "    print(i['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6303e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test_dataloader:\n",
    "#     print(i['input_values'].shape)\n",
    "#     print(i['attention_mask'].shape)\n",
    "#     print(i['labels'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807ded8",
   "metadata": {},
   "source": [
    "### Loading the Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45b2caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2Model.from_pretrained(BASE_MODEL_ID)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad1dd943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48eed5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec5c3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods = [method for method in dir(model) if callable(getattr(model, method))]\n",
    "# print(methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a99eb707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "    (attention): Wav2Vec2SdpaAttention(\n",
       "      (k_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "      (v_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "      (q_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "      (out_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "    (feed_forward): Wav2Vec2FeedForward(\n",
       "      (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (intermediate_dense): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (output_dense): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "      (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a2aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 48 transformer layers in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267cb74",
   "metadata": {},
   "source": [
    "### Currently i am training only My custom CTC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57da4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze feature extractor (same as model.freeze_feature_encoder())\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze feature projection (optional)\n",
    "for param in model.feature_projection.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze all transformer encoder layers\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "    \n",
    "# Freeze first 30 transformer layers (out of 48 for base model)\n",
    "# for i in range(48):\n",
    "#     for param in model.encoder.layers[i].parameters():\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "        \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "# Optional: Print which layers are frozen\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb398821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 0 / Total: 2159259648\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable_params} / Total: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "339ed06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f870766",
   "metadata": {},
   "source": [
    "### Building the custom CTC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6a587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68121554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, model, projection_dim = 5000):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = model\n",
    "        self.projection = nn.Linear(1920, projection_dim)\n",
    "\n",
    "    def forward(self, input_values, attention_mask = None):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask = attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [batch, time, hidden]\n",
    "        projected = self.projection(hidden_states)  # [batch, time, 5000]\n",
    "        return projected\n",
    "\n",
    "\n",
    "# Custom CTC model\n",
    "class CustomWav2Vec2CTC(nn.Module):\n",
    "    def __init__(self, model, vocab_size, projection_dim = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.projector = Projector(model, projection_dim = projection_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(projection_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_values, attention_mask = None):\n",
    "        hidden_states = self.projector(input_values, attention_mask)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afd4bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3147"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_files/multilingual_vocab.json\"\n",
    "\n",
    "# Open and load JSON data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ce38133",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3147  # your vocab size here\n",
    "\n",
    "# This is complete model\n",
    "\n",
    "multilingual_asr_model = CustomWav2Vec2CTC(model, vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0657b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_input = torch.randn(1, 16000).to(device)  # simulate 1 sec of audio\n",
    "\n",
    "# logits = multilingual_asr_model(dummy_input)\n",
    "\n",
    "# print(logits.shape)  # → (batch_size, time_steps, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a928fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 25343147 / Total: 2184602795\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in multilingual_asr_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in multilingual_asr_model.parameters())\n",
    "print(f\"Trainable: {trainable_params} / Total: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce1869",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1c65d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(labels, preds):\n",
    "#     preds = torch.argmax(preds, axis=-1)\n",
    "\n",
    "#     # Clone to avoid modifying the original labels used for loss\n",
    "#     labels_clean = labels.clone()\n",
    "#     labels_clean[labels_clean == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "#     pred_str = processor.batch_decode(preds)\n",
    "#     label_str = processor.batch_decode(labels_clean, group_tokens=False)\n",
    "\n",
    "#     return wer(label_str, pred_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e57ce191",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(multilingual_asr_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c14f9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of training steps the model will take during the entire training process.\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_scheduler\n",
    "# SchedulerType, please select one of ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau', 'cosine_with_min_lr', 'warmup_stable_decay']\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "                            \"linear\",\n",
    "                            optimizer = optimizer,\n",
    "                            num_warmup_steps = 2,\n",
    "                            num_training_steps = num_training_steps\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84127811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375030"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mixed training of torch.float32, torch.float64\n",
    "# https://pytorch.org/docs/stable/amp.html\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda:0\")\n",
    "\n",
    "multilingual_asr_model = multilingual_asr_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9144f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version = 1\n",
    "# model_save_path = \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_weights\"\n",
    "# model_name = f\"multilingual_asr_{version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e309759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(multilingual_asr_model.state_dict(), os.path.join(model_save_path, model_name + \".pt\"))\n",
    "# processor.save_pretrained(os.path.join(model_save_path, model_name + \"_vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68b19a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step, data in enumerate(train_dataloader):\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "    \n",
    "#     # moving inputs to device\n",
    "#     batch = {k: v.to(device) for k, v in data.items()}\n",
    "\n",
    "#     for i in batch:\n",
    "#         print(i)\n",
    "#         print(batch[i].dtype)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "138d46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40c5d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "ctc_loss_fn = nn.CTCLoss(blank = processor.tokenizer.pad_token_id, zero_infinity = True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88c78f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(train_dataloader, optimizer, lr_scheduler):\n",
    "\n",
    "#     losses = []\n",
    "#     wers = []\n",
    "\n",
    "#     for step, batch in enumerate(tqdm(train_dataloader, desc = \"Training\", position = 1, leave = True)):\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "#         with torch.autocast(\"cuda\"):\n",
    "#             logits = multilingual_asr_model(batch[\"input_values\"], attention_mask = batch.get(\"attention_mask\"))\n",
    "            \n",
    "#             # print('The shape of the logits', logits.shape)\n",
    "\n",
    "#         # logits shape: (batch, time, vocab) → required shape: (time, batch, vocab)\n",
    "#         log_probs = F.log_softmax(logits, dim = -1).transpose(0, 1)\n",
    "\n",
    "#         # print('The shape of the log_probs', log_probs.shape)\n",
    "        \n",
    "#         input_lengths = torch.full(\n",
    "#                                         size = (log_probs.size(1),),  # batch size\n",
    "#                                         fill_value = log_probs.size(0),  # time dimension\n",
    "#                                         dtype = torch.long,\n",
    "#                                     ).to(device)\n",
    "\n",
    "#         # print('The input lengths', input_lengths)\n",
    "#         # print('The shape of the input lengths', input_lengths.shape)\n",
    "        \n",
    "#         # Prepare labels\n",
    "#         labels = batch[\"labels\"]\n",
    "#         # print('The shape of the labels is', labels.shape)\n",
    "        \n",
    "#         target_lengths = (labels != -100).sum(dim = 1)\n",
    "#         # print('The target_lengths is', target_lengths)\n",
    "\n",
    "#         # Replace pad_token_id with -100 for CTC compatibility\n",
    "#         flattened_targets = labels[labels != -100]\n",
    "#         # print('The flattened_targets is', flattened_targets)\n",
    "        \n",
    "        \n",
    "#         # -100 doesn’t go into the loss.\n",
    "#         with torch.cuda.amp.autocast(enabled = False):\n",
    "#             loss = ctc_loss_fn(\n",
    "#                                 log_probs.float(),         # ensure float32\n",
    "#                                 flattened_targets,\n",
    "#                                 input_lengths,\n",
    "#                                 target_lengths\n",
    "#                                 )\n",
    "#         # loss = ctc_loss_fn(\n",
    "#         #                         log_probs,             # (T, N, C)\n",
    "#         #                         flattened_targets,     # (sum of target lengths)\n",
    "#         #                         input_lengths,         # (N)\n",
    "#         #                         target_lengths         # (N)\n",
    "#         #                     )\n",
    "        \n",
    "#         # print('The loss is', loss)\n",
    "        \n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         lr_scheduler.step()\n",
    "#         scaler.update()\n",
    "\n",
    "#         # WER computation\n",
    "#         preds = torch.argmax(log_probs, dim = -1).transpose(0, 1)  # back to (batch, time)\n",
    "#         # print('The preds is', preds)\n",
    "#         # print('The labels is', labels)\n",
    "        \n",
    "#         metrics = compute_metrics(labels, preds)\n",
    "#         wers.append(metrics)\n",
    "        \n",
    "#         # print('The metrics is', metrics)\n",
    "        \n",
    "#         # break\n",
    "\n",
    "#     return {\"loss\": np.mean(losses), \"wer\": np.mean(wers)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555d911",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5a951c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['採p浜', 'Ô']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode([[1000, 20, 1212], [45]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cb8a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels, preds):\n",
    "    \n",
    "    # preds = torch.argmax(preds, axis=-1)\n",
    "\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # print('The shape of the preds is', preds)\n",
    "    # print('The shape of the label is', labels)\n",
    "    pred_str = processor.batch_decode(preds)\n",
    "    \n",
    "    # print('The pred str is', pred_str)\n",
    "    label_str = processor.batch_decode(labels, group_tokens = False)\n",
    "    # print('The label str is', label_str)\n",
    "    return wer(label_str, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd1613e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01f4a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the CSV\n",
    "csv_file_path = \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_loss_files/loss_0_2_lakh/loss_0_2_lakh.csv\"\n",
    "\n",
    "# Initialize CSV file with headers if it doesn't exist\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"step\", \"epoch\", \"loss\", \"wer\"])\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08a408",
   "metadata": {},
   "source": [
    "### Checkpoint saving variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f93d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed training from epoch 15 with best loss 3.7793\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "checkpoint_dir = \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_weights/weights_0_2_lakh\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "start_epoch = 0\n",
    "curr_best_loss = 1e9\n",
    "resume_training = True  # set this to False to start fresh\n",
    "\n",
    "model_name = \"multilingual_asr_model_0_2_lakh\"\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, model_name + \".pt\")\n",
    "\n",
    "# Resume logic\n",
    "if resume_training and os.path.exists(checkpoint_path):\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    multilingual_asr_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    \n",
    "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    curr_best_loss = checkpoint.get('best_loss', curr_best_loss)\n",
    "    \n",
    "    print(f\"Resumed training from epoch {start_epoch} with best loss {curr_best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2dd8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_asr_model = multilingual_asr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7116c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move optimizer state to the correct device\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c0583e4a2e439b88981c5e793d7af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0bef8bf5e3443c8f8b5cd684e286c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "curr_best_loss = 1e9\n",
    "\n",
    "for n in tqdm(range(start_epoch, epochs)):\n",
    "    \n",
    "    losses = []\n",
    "    wers = []\n",
    "\n",
    "    total_number_batch = len(train_dataloader)\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # print('the step is ', step)\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            logits = multilingual_asr_model(batch[\"input_values\"], attention_mask = batch.get(\"attention_mask\"))\n",
    "            \n",
    "            # print('The shape of the logits', logits.shape)\n",
    "\n",
    "        # logits shape: (batch, time, vocab) → required shape: (time, batch, vocab)\n",
    "        log_probs = F.log_softmax(logits, dim = -1).transpose(0, 1)\n",
    "        \n",
    "        # print('The log_probs is', log_probs)\n",
    "        # print('The shape of the log_probs', log_probs.shape)\n",
    "        \n",
    "        input_lengths = torch.full(\n",
    "                                    size = (log_probs.size(1),),  # batch size\n",
    "                                    fill_value = log_probs.size(0),  # time dimension\n",
    "                                    dtype = torch.long).to(device)\n",
    "\n",
    "        # print('The input lengths', input_lengths)\n",
    "        # print('The shape of the input lengths', input_lengths.shape)\n",
    "        \n",
    "        # Prepare labels\n",
    "        labels = batch[\"labels\"]\n",
    "        # print('The shape of the labels is', labels.shape)\n",
    "        \n",
    "        target_lengths = (labels != -100).sum(dim = 1)\n",
    "        # print('The target_lengths is', target_lengths)\n",
    "\n",
    "        # Replace pad_token_id with -100 for CTC compatibility\n",
    "        flattened_targets = labels[labels != -100]\n",
    "        # print('The flattened_targets is', flattened_targets)\n",
    "        \n",
    "        \n",
    "        # -100 doesn’t go into the loss.\n",
    "        with torch.cuda.amp.autocast(enabled = False):\n",
    "            loss = ctc_loss_fn(\n",
    "                                log_probs.float(),         # ensure float32\n",
    "                                flattened_targets,\n",
    "                                input_lengths,\n",
    "                                target_lengths\n",
    "                                )\n",
    "        # loss = ctc_loss_fn(\n",
    "        #                         log_probs,             # (T, N, C)\n",
    "        #                         flattened_targets,     # (sum of target lengths)\n",
    "        #                         input_lengths,         # (N)\n",
    "        #                         target_lengths         # (N)\n",
    "        #                     )\n",
    "        \n",
    "        # print('The loss is', loss)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        lr_scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "        # WER computation\n",
    "        preds = torch.argmax(log_probs, dim = -1).transpose(0, 1)  # back to (batch, time)\n",
    "        \n",
    "        \n",
    "        # print('The shape of preds is', preds.shape)\n",
    "        # print('The labels is', labels)\n",
    "        \n",
    "        labels_for_metrics = labels.clone()\n",
    "        \n",
    "        metrics = compute_metrics(labels_for_metrics, preds)\n",
    "        wers.append(metrics)\n",
    "        \n",
    "        \n",
    "        with open(csv_file_path, mode = 'a', newline = '') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"step\", \"epoch\", \"loss\", \"wer\"])\n",
    "            \n",
    "            writer.writerow({\n",
    "                                \"step\": step,\n",
    "                                \"epoch\": n + 1,\n",
    "                                \"loss\": loss.item(),\n",
    "                                \"wer\": metrics\n",
    "                            })\n",
    "\n",
    "    result = {\"loss\": np.mean(losses), \"wer\": np.mean(wers)}    \n",
    "    \n",
    "    print(\"EPOCH: \", n + 1)\n",
    "    print(result)\n",
    "    print('=' * 100)\n",
    "    \n",
    "        \n",
    "    if result[\"loss\"] < curr_best_loss:\n",
    "        \n",
    "        print(f\"New best model found at epoch {n + 1} with loss {result['loss']:.4f}, saving...\")\n",
    "        \n",
    "        curr_best_loss = result[\"loss\"]\n",
    "    \n",
    "    \n",
    "    # Save latest checkpoint for every epoch\n",
    "    checkpoint = {\n",
    "                        'epoch': n + 1,\n",
    "                        'model_state_dict': multilingual_asr_model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                        'loss': result['loss'],\n",
    "                        'best_loss': curr_best_loss\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    \n",
    "    if result[\"loss\"] < curr_best_loss:\n",
    "        \n",
    "        print(f\"New best model found at epoch {n + 1} with loss {result['loss']:.4f}, saving...\")\n",
    "        \n",
    "        # best_checkpoint = {\n",
    "        #                         'epoch': n + 1,\n",
    "        #                         'model_state_dict': multilingual_asr_model.state_dict(),\n",
    "        #                         'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #                         'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        #                         'loss': result['loss']\n",
    "        #                     }\n",
    "        \n",
    "        # torch.save(best_checkpoint, os.path.join(checkpoint_path, model_name + \"best\" + \".pt\"))\n",
    "        \n",
    "        # Update best loss\n",
    "        curr_best_loss = result[\"loss\"]\n",
    "\n",
    "print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b9c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

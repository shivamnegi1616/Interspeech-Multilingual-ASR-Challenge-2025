{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6bda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jiwer import wer\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Audio\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# AdamW is best optimizer\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2e5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"VOCAB_PATH\" : \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/model_code/model_files/multilingual_vocab.json\",\n",
    "          \"DEVICE\" : 0,\n",
    "          \"BASE_MODEL_ID\" : \"facebook/wav2vec2-xls-r-2b\",\n",
    "          \n",
    "          \"infer_checkpoint_dir\" : \"/scratch/IITB/ai-at-ieor/23m1508/23m1508_backup/checkpoint_points_2/weights_401000_406000/multilingual_asr_model_401000_406000.pt\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33eab681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(f\"cuda:{config[\"DEVICE\"]}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad6654",
   "metadata": {},
   "source": [
    "### Model Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3db042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the tokenizer\n",
    "tokenizer = Wav2Vec2CTCTokenizer(config['VOCAB_PATH'],\n",
    "                                 bos_token = \"<s>\",\n",
    "                                 eos_token = \"</s>\",\n",
    "                                 unk_token = \"<unk>\", \n",
    "                                 pad_token = \"<pad>\", \n",
    "                                 word_delimiter_token = \"|\")\n",
    "\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size = 1, \n",
    "                                             sampling_rate = 16000, \n",
    "                                             padding_value = 0.0, \n",
    "                                             do_normalize = True, \n",
    "                                             return_attention_mask = True)\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor = feature_extractor, \n",
    "                              tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e780db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaidng the pretrained model\n",
    "model = Wav2Vec2Model.from_pretrained(config['BASE_MODEL_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45cf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, model, projection_dim = 5000):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = model\n",
    "        self.projection = nn.Linear(1920, projection_dim)\n",
    "\n",
    "    def forward(self, input_values, attention_mask = None):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask = attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [batch, time, hidden]\n",
    "        projected = self.projection(hidden_states)  # [batch, time, 5000]\n",
    "        return projected\n",
    "\n",
    "\n",
    "# Custom CTC model\n",
    "class CustomWav2Vec2CTC(nn.Module):\n",
    "    def __init__(self, model, vocab_size, projection_dim = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.projector = Projector(model, projection_dim = projection_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(projection_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_values, attention_mask = None):\n",
    "        hidden_states = self.projector(input_values, attention_mask)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42ac5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your JSON file\n",
    "file_path = config['VOCAB_PATH']\n",
    "\n",
    "# Open and load JSON data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "vocab_size = len(vocab)  # your vocab size here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "726c5e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3147"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490aa990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is complete model\n",
    "multilingual_asr_model = CustomWav2Vec2CTC(model, vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf96f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_best_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d69d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from /scratch/IITB/ai-at-ieor/23m1508/23m1508_backup/checkpoint_points_2/weights_401000_406000/multilingual_asr_model_401000_406000.pt\n",
      "Resuming at epoch 254, best loss: 1.0442\n",
      "Loaded form the previous model: \n",
      "Loaded checkpoint from /scratch/IITB/ai-at-ieor/23m1508/23m1508_backup/checkpoint_points_2/weights_401000_406000/multilingual_asr_model_401000_406000.pt\n",
      "Trained till to epoch 254 with best loss 1.0442\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = config['infer_checkpoint_dir']\n",
    "\n",
    "map_location = device\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location = 'cpu', weights_only = False)\n",
    "multilingual_asr_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_state = checkpoint['optimizer_state_dict']\n",
    "scheduler_state = checkpoint['scheduler_state_dict']\n",
    "start_epoch = checkpoint['epoch']\n",
    "curr_best_loss = checkpoint.get('best_loss', curr_best_loss)\n",
    "\n",
    "print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "print(f\"Resuming at epoch {start_epoch}, best loss: {curr_best_loss:.4f}\")\n",
    "print('Loaded form the previous model: ')\n",
    "print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "print(f\"Trained till to epoch {start_epoch} with best loss {curr_best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc1fb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomWav2Vec2CTC(\n",
       "  (projector): Projector(\n",
       "    (wav2vec2): Wav2Vec2Model(\n",
       "      (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): Wav2Vec2FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=1920, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "        (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1920, 1920, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (padding): Wav2Vec2SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "            (attention): Wav2Vec2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "              (v_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "              (q_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "              (out_proj): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): Wav2Vec2FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projection): Linear(in_features=1920, out_features=5000, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=5000, out_features=3147, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_asr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e4d949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(multilingual_asr_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d67af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, target_sr = 16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    if sample_rate != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    inputs = processor.feature_extractor(\n",
    "                                            waveform,\n",
    "                                            sampling_rate=target_sr,\n",
    "                                            return_tensors=\"pt\"\n",
    "                                        )\n",
    "    \n",
    "    inputs = inputs[\"input_values\"]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2aa31",
   "metadata": {},
   "source": [
    "### Final Inference Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02e864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, file_path):\n",
    "    \n",
    "    input_values = preprocess_audio(file_path).to(device)  # (1, time)\n",
    "    # print('the shape of the input_values is', input_values.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values)\n",
    "        log_probs = F.log_softmax(logits, dim = -1)\n",
    "\n",
    "        predicted_ids = torch.argmax(log_probs, dim = -1)  # (batch, time)\n",
    "\n",
    "        transcription = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "        \n",
    "        return transcription[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ea9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/scratch/IITB/ai-at-ieor/23m1508/23m1508_backup/Evaluation_Set_1/All_Evaluation_audio/Thai-0245_003_phone-O2-128859-128985.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aed88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer(multilingual_asr_model, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544e98dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_path = \"/scratch/IITB/ai-at-ieor/23m1508/23m1508_backup/Evaluation_Set_1/evaluation_paths.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a883a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French-0185_004_phone-O2-000779-000914</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French-0185_004_phone-O2-001194-001283</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>French-0185_004_phone-O1-001324-001450</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>French-0185_004_phone-O2-001467-001585</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French-0185_004_phone-O1-001651-001737</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29294</th>\n",
       "      <td>Thai-0245_003_phone-O2-128309-128491</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29295</th>\n",
       "      <td>Thai-0245_003_phone-O1-128502-128660</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29296</th>\n",
       "      <td>Thai-0245_003_phone-O2-128661-128699</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29297</th>\n",
       "      <td>Thai-0245_003_phone-O1-128700-128843</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29298</th>\n",
       "      <td>Thai-0245_003_phone-O2-128859-128985</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "      <td>/scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29299 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ID  \\\n",
       "0      French-0185_004_phone-O2-000779-000914   \n",
       "1      French-0185_004_phone-O2-001194-001283   \n",
       "2      French-0185_004_phone-O1-001324-001450   \n",
       "3      French-0185_004_phone-O2-001467-001585   \n",
       "4      French-0185_004_phone-O1-001651-001737   \n",
       "...                                       ...   \n",
       "29294    Thai-0245_003_phone-O2-128309-128491   \n",
       "29295    Thai-0245_003_phone-O1-128502-128660   \n",
       "29296    Thai-0245_003_phone-O2-128661-128699   \n",
       "29297    Thai-0245_003_phone-O1-128700-128843   \n",
       "29298    Thai-0245_003_phone-O2-128859-128985   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "1      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "2      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "3      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "4      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "...                                                  ...   \n",
       "29294  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "29295  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "29296  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "29297  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "29298  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...   \n",
       "\n",
       "                                                    Path  \n",
       "0      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "1      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "2      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "3      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "4      /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "...                                                  ...  \n",
       "29294  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "29295  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "29296  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "29297  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "29298  /scratch/IITB/ai-at-ieor/23m1508/23m1508_backu...  \n",
       "\n",
       "[29299 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_data = pd.read_csv(evaluation_path)\n",
    "evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dd5b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_inference_on_dataframe(df, model, output_path, device):\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # f.write(\"UtteranceID Hypothesis\\n\")  # header\n",
    "        \n",
    "        for i, row in tqdm(df.iterrows(), total = len(df), desc = \"Inference Progress\"):\n",
    "            \n",
    "            utt_id = row[\"ID\"]\n",
    "            audio_path = row[\"Path\"]\n",
    "\n",
    "            try:\n",
    "                transcription = infer(model, audio_path)\n",
    "            except Exception as e:\n",
    "                transcription = f\"[ERROR: {str(e)}]\"\n",
    "                print(transcription)\n",
    "            # print(transcription)\n",
    "            f.write(f\"{utt_id} {transcription.strip()}\\n\")\n",
    "            f.flush()  # <-- Force immediate write to disk\n",
    "            \n",
    "            # Explicit GPU memory cleanup\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04a23a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3e0fddc8c346bf951f3d604c388032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Progress:   0%|          | 0/29299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_inference_on_dataframe(df = evaluation_data, \n",
    "                           model = multilingual_asr_model, \n",
    "                           output_path = \"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/submission_code/submission_csv/submission_csv_ap1.txt\", \n",
    "                           device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d7db9",
   "metadata": {},
   "source": [
    "### Making the (text_space) with no extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc46e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def add_space_between_chars(text):\n",
    "    pattern = re.compile(\n",
    "        r\"([\\u1100-\\u11ff\\u2e80-\\ua4cf\\ua840-\\uD7AF\\uF900-\\uFAFF\\uFE30-\\uFE4F\\uFF65-\\uFFDC\\U00020000-\\U0002FFFF\\u3000-\\u303F\\uff01-\\uff60\\u0E00-\\u0E7F])\"\n",
    "    )  # CJKT + Thai characters\n",
    "    chars = pattern.split(text)\n",
    "    chars = [ch for ch in chars if ch.strip()]\n",
    "    text = \" \".join(w for w in chars)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adeaa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/submission_code/submission_csv/submission_csv_ap1.txt\")  # your input file path\n",
    "\n",
    "output_path = Path(\"/home/IITB/ai-at-ieor/23m1508/Shivam_23M1508/Interspeech/code/submission_code/submission_csv/text_space\")     # output file name without extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac007402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3204fb3e4b824d6fae9df7ae62abe74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    \n",
    "    for line in tqdm(fin):\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        utt = line.split()[0]\n",
    "        text = ' '.join(line.split()[1:])\n",
    "        \n",
    "        if 'Japanese' in utt or 'Korean' in utt or 'Thai' in utt:\n",
    "            text = add_space_between_chars(text)\n",
    "            \n",
    "        fout.write(f\"{utt} {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd1001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e354a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shivam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
